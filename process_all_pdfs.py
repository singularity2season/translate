import requests
import os
import glob
import xml.etree.ElementTree as ET
import time
from dotenv import load_dotenv
from docx import Document # è¿½åŠ : Wordä½œæˆç”¨
from docx.shared import Pt # è¿½åŠ : ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºæŒ‡å®šç”¨

# --- â–¼ è¨­å®šã‚¨ãƒªã‚¢ â–¼ ---

load_dotenv()

DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")
DEEPL_URL = os.getenv("DEEPL_API_URL", "https://api-free.deepl.com/v2/translate")
GROBID_URL = os.getenv("GROBID_API_URL", "http://localhost:8070/api/processFulltextDocument")

TARGET_LANG = "JA"
GROBID_TIMEOUT = 180
INPUT_DIR = "input_pdf"
OUTPUT_DIR = "output_pdf"

# å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€è¨­å®š
OUTPUT_XML_DIR = os.path.join(OUTPUT_DIR, "xml")
OUTPUT_DOCX_DIR = os.path.join(OUTPUT_DIR, "docx") # å¤‰æ›´: docxç”¨ãƒ•ã‚©ãƒ«ãƒ€
NAMESPACES = {'tei': 'http://www.tei-c.org/ns/1.0'}

# ------------------------------------------------

def setup_directories():
    os.makedirs(OUTPUT_XML_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DOCX_DIR, exist_ok=True)

def translate_text_via_deepl(text):
    """DeepL APIã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³"""
    if not text or not text.strip():
        return ""
    
    params = {
        "auth_key": DEEPL_API_KEY,
        "text": text,
        "target_lang": TARGET_LANG
    }

    try:
        response = requests.post(DEEPL_URL, data=params, timeout=30)
        if response.status_code == 200:
            return response.json()["translations"][0]["text"]
        elif response.status_code == 456:
            return "[Translation Error: Quota Exceeded]"
        else:
            print(f"  âš ï¸ DeepLã‚¨ãƒ©ãƒ¼: {response.status_code}")
            return text # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åŸæ–‡ã‚’è¿”ã™
    except Exception as e:
        print(f"  âš ï¸ é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        return text

def translate_long_text(full_text):
    """é•·æ–‡ã‚’æ®µè½ã”ã¨ã«ç¿»è¨³ã—ã¦çµåˆ"""
    paragraphs = full_text.split("\n\n")
    translated_paragraphs = []
    
    print(f"  ğŸ¤– æœ¬æ–‡ç¿»è¨³ä¸­: å…¨ {len(paragraphs)} æ®µè½...")

    for i, para in enumerate(paragraphs):
        if not para.strip(): continue
        trans = translate_text_via_deepl(para)
        translated_paragraphs.append(trans)
        time.sleep(0.5) # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        if (i + 1) % 10 == 0:
            print(f"    ... {i + 1}/{len(paragraphs)} å®Œäº†")

    return "\n\n".join(translated_paragraphs)

# --- â–¼ XMLè§£ææ©Ÿèƒ½ã®å¼·åŒ– â–¼ ---

def extract_data_from_xml(xml_content):
    """XMLã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã€æœ¬æ–‡ã€å‚è€ƒæ–‡çŒ®ã‚’æŠ½å‡ºã™ã‚‹"""
    try:
        root = ET.fromstring(xml_content)
        
        # 1. ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
        title_node = root.find('.//tei:teiHeader//tei:titleStmt/tei:title', NAMESPACES)
        title = title_node.text.strip() if title_node is not None and title_node.text else "No Title Found"

        # 2. æœ¬æ–‡æŠ½å‡º (æ®µè½ã”ã¨)
        body_text_list = []
        paragraphs = root.findall('.//tei:text//tei:p', NAMESPACES)
        for p in paragraphs:
            # itertext()ã§ã™ã¹ã¦ã®ã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            text = "".join(p.itertext()).strip()
            if text:
                body_text_list.append(text)
        full_body = "\n\n".join(body_text_list)

        # 3. å‚è€ƒæ–‡çŒ®æŠ½å‡º
        references = []
        bib_structs = root.findall('.//tei:listBibl/tei:biblStruct', NAMESPACES)
        
        for i, bib in enumerate(bib_structs, 1):
            # ç°¡æ˜“çš„ãªæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯: ã‚¿ã‚¤ãƒˆãƒ«ã¨è‘—è€…ãªã©ã‚’ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦çµåˆ
            # æœ¬æ¥ã¯ç´°ã‹ãã‚¿ã‚°ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã¹ãã ãŒã€GROBIDã®å‡ºåŠ›æ§‹é€ ã«åˆã‚ã›ã¦ç°¡æ˜“åŒ–
            ref_text_parts = []
            
            # ã‚¿ã‚¤ãƒˆãƒ« (è«–æ–‡å or æ›¸ç±å)
            ref_title = bib.find('.//tei:title', NAMESPACES)
            if ref_title is not None and ref_title.text:
                ref_text_parts.append(f"\"{ref_title.text}\"")
            
            # ç™ºè¡Œå¹´
            date = bib.find('.//tei:date', NAMESPACES)
            if date is not None and date.get('when'):
                ref_text_parts.append(f"({date.get('when')})")
            
            # é›‘èªŒåãªã©
            pub = bib.find('.//tei:publicationStmt/tei:publisher', NAMESPACES)
            if pub is not None and pub.text:
                ref_text_parts.append(pub.text)

            # ã‚‚ã—æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãŒã†ã¾ãå–ã‚Œãªã‘ã‚Œã°ã€noteã‚¿ã‚°ãªã©ã‚’æ¢ã™ï¼ˆç°¡æ˜“å¯¾å¿œï¼‰
            full_ref_str = " ".join(ref_text_parts)
            if not full_ref_str:
                full_ref_str = "Extraction Failed"
            
            references.append(f"[{i}] {full_ref_str}")

        return {
            "title": title,
            "body": full_body,
            "references": references
        }

    except Exception as e:
        print(f"  âŒ XMLè§£æã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- â–¼ Wordç”Ÿæˆæ©Ÿèƒ½ â–¼ ---

def create_word_document(data, output_path):
    """ç¿»è¨³çµæœã‚’Wordãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
    doc = Document()

    # 1. ã‚¿ã‚¤ãƒˆãƒ« (æ—¥æœ¬èª + è‹±èª)
    doc.add_heading(data['jp_title'], 0) # å¤§ããªè¦‹å‡ºã—
    subtitle = doc.add_paragraph(data['en_title'])
    subtitle.italic = True # åŸæ–‡ã‚¿ã‚¤ãƒˆãƒ«ã¯æ–œä½“ã§

    # 2. æœ¬æ–‡ (æ—¥æœ¬èª)
    doc.add_heading('æœ¬æ–‡ (Translated)', level=1)
    
    # æ®µè½ã”ã¨ã«Wordã®ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã¨ã—ã¦è¿½åŠ ï¼ˆèª­ã¿ã‚„ã™ã•ã®ãŸã‚ï¼‰
    paragraphs = data['jp_body'].split("\n\n")
    for p_text in paragraphs:
        p = doc.add_paragraph(p_text)
        p.paragraph_format.space_after = Pt(12) # æ®µè½å¾Œã®ä½™ç™½

    # 3. å‚è€ƒæ–‡çŒ® (åŸæ–‡ã®ã¾ã¾)
    if data['references']:
        doc.add_page_break() # æ”¹ãƒšãƒ¼ã‚¸
        doc.add_heading('å‚è€ƒæ–‡çŒ® (References)', level=1)
        for ref in data['references']:
            doc.add_paragraph(ref, style='List Number')

    doc.save(output_path)
    print(f"  ğŸ’¾ Wordä¿å­˜å®Œäº†: {os.path.basename(output_path)}")

# --- â–¼ ãƒ¡ã‚¤ãƒ³å‡¦ç† â–¼ ---

def process_single_pdf(pdf_path):
    base_filename = os.path.basename(pdf_path).replace(".pdf", "")
    xml_path = os.path.join(OUTPUT_XML_DIR, f"{base_filename}.xml")
    docx_path = os.path.join(OUTPUT_DOCX_DIR, f"{base_filename}_translated.docx")

    # ç¿»è¨³æ¸ˆã¿(docxãŒå­˜åœ¨ã™ã‚‹)ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
    if os.path.exists(docx_path):
        print(f"\nâ­ï¸  å®Œå…¨ã‚¹ã‚­ãƒƒãƒ— (å®Œäº†æ¸ˆã¿): {base_filename}")
        return

    print(f"\nğŸ”„ å‡¦ç†é–‹å§‹: {base_filename}")

    # 1. GROBIDå®Ÿè¡Œ & XMLä¿å­˜
    xml_content = ""
    if os.path.exists(xml_path):
        print("  ğŸ“‚ æ—¢å­˜ã®XMLã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        with open(xml_path, "r", encoding="utf-8") as f:
            xml_content = f.read()
    else:
        try:
            with open(pdf_path, 'rb') as f:
                files = {'input': f}
                resp = requests.post(GROBID_URL, files=files, data={'consolidateHeader': '1', 'consolidateCitations': '1'}, timeout=GROBID_TIMEOUT)
            
            if resp.status_code != 200:
                print(f"  âŒ GROBIDã‚¨ãƒ©ãƒ¼: {resp.status_code}")
                return
            
            xml_content = resp.text
            with open(xml_path, "w", encoding="utf-8") as f:
                f.write(xml_content)
            print("  âœ… PDFè§£æå®Œäº† (GROBID)")
        except Exception as e:
            print(f"  âŒ GROBIDæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            return

    # 2. XMLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    extracted_data = extract_data_from_xml(xml_content)
    if not extracted_data or not extracted_data['body']:
        print("  âš ï¸ æœ¬æ–‡æŠ½å‡ºå¤±æ•—")
        return

    # 3. ç¿»è¨³ (ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡)
    print(f"  ğŸŒ ã‚¿ã‚¤ãƒˆãƒ«ç¿»è¨³ä¸­: {extracted_data['title'][:30]}...")
    jp_title = translate_text_via_deepl(extracted_data['title'])
    
    jp_body = translate_long_text(extracted_data['body'])

    # 4. Wordç”Ÿæˆç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    doc_data = {
        "en_title": extracted_data['title'],
        "jp_title": jp_title,
        "jp_body": jp_body,
        "references": extracted_data['references'] # å‚è€ƒæ–‡çŒ®ã¯ç¿»è¨³ã—ãªã„
    }

    # 5. Wordä¿å­˜
    create_word_document(doc_data, docx_path)

def main():
    setup_directories()
    pdf_files = glob.glob(os.path.join(INPUT_DIR, "*.pdf"))
    
    if not pdf_files:
        print(f"'{INPUT_DIR}' ã«PDFãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"--- {len(pdf_files)} ä»¶ã®PDFã‚’Wordå¤‰æ›ã—ã¾ã™ ---")
    
    for pdf in pdf_files:
        process_single_pdf(pdf)

    print("\n--- å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ ---")

if __name__ == "__main__":
    main()